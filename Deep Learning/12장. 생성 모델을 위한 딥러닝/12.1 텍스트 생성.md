순환 신경망으로 시퀀스 데이터를 생성할 수 있다.

#12.1.1 시퀀스 생성을 위한 딥러닝 모델의 간단한 역사

2002년 : 스위스의 슈미드후버의 연구실에서 LSTM을 음악 생성에 처음 적용하여 가능성 있는 결과를 얻음
2000년대 후반 2010년 초반 알렉스 그레이브스는 순환 네트워크를 사용하여 시퀀스 데이터를 생성하는데 아주 중요한 성과를 냄. 
2013년 : 펜 위치를 기록한 시계열 데이터를 사용해서 순환 네트워크와 완전 연결 네트웤를 혼합한 네트워크로 사람이 쓴 것 같은 손글씨를 생성.
2015년 ~ 2017년: 순환 신경망은 텍스트와 대화 생성, 음악 생성, 음성 합성에 성공적으로 사용
2017년 ~ 2018년: 트랜스포머 아키텍처가 자연어 처리 지도 학습 작업뿐만 아니라 시퀀스 생성 모델, 특히 언어 모델링에서 순환 신경망을 압도함. -> chat GPT-3

#12.1.2 시퀀스 데이털르 어떻게 생성할까?
시퀀스 데이터를 생성하는 방법
-> 이전 토큰을 입력으로 사용해서 시퀀스의 다음 1개 또는 몇 개의 토큰을 예측하는것

언어 모델(language model): 이전 토큰들이 주어졌을 때 다음 토큰의 확률을 모델링할 수 있는 네트워크
![[스크린샷 2025-02-13 오전 12.11.17.png]]

#12.1.3 샘플링 전략의 중요성
텍스트를 생성할 때 다음 문자를 선택하는 방법은 중요함!!
1. 탐욕적 샘플링: 가장 높은 확률을 가진 글자를 선택
2. 확률적 샘플링: 확률 분포에서 샘플링하는 과정에 무작위성을 주입해서 선택

엔트로피가 클때 : 놀랍고 창의적인 시퀀스 생성
엔트로피가 작을때 : 예상가능한 구조를 가진 시퀀스 생성

샘플링 과정에서 확률의 양을 조절하기 위해 소프트맥스 온도라는 파라미터를 사용한다.
-> 이 파라미터는 샘플리에 사용되는 확률 분포의 엔트로피를 나타낸다.

[코드12-1 다른 온도 값을 사용하여 확률 분포의 가중치 바꾸기]
```
import numpy as np
	def reweight_distribution(original_distribution, temperature=0.5):
	distribution = np.log(original_distribution) / temperature
	distribution = np.exp(distribution)
	return distribution / np.sum(distribution)
```


높은 온도 -> 높은 엔트로피 생성 -> 놀랍고 생소한 데이터 생성
낮은 온도 -> 낮은 엔트로피 생성 -> 예상 가능한 데이터 생성
![[스크린샷 2025-02-13 오전 12.26.03.png]]

#12.1.4 케라스를 사용한 텍스트 생성 모델 구현


