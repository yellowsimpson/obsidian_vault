#1.2.1 확률적 모델링
확률적 모델링은 통계학 이론을 데이터 분석에 응용한 것이다.
ex) 
#나이브베이즈알고리즘(Bayess' theorem) -> 입력 데이터의 특성이 모두 독립적이라고 가정하고 베이즈 정리를 적용하는 머신 러닝 분류 알고리즘
#로지스틱회귀(logistic regression) -> 분류(classification) 알고리즘

#1.2.2 초차기 신경망
성공적인 첫번째 신경망은 1989년 벨 연구소에서 나왔다.
얀 르쿤(Yann LeCun)은 초창기 합성곱 신경망과 역전파를 연결하여 손글씨 숫자 이미지를 분류하는 문제에 적용
->LeNet이라고 불리는 이 신경망은 우편 봉투의 우편 번호 코드를 자동으로 읽기 위해 1990년대 미국 우편 서비스에 사용

#1.2.3 커널 방법
*커널 방법(Kernel method)* : 분류 알고리즘의 한 종류를 말하며 그중 서포트 벡터 머신SVM(Support Vector Machine)이 가장 유명함

*SVM(Support Vector Machine)*: 2개의 class를 나누는 결정 경계를 찾는 분류 알고리즘

*SVM을 찾는 과정*
1. 결정 경계가 하나의 초평면으로 표현될 수 있는 새로운 고차원 표현으로 데이터를 매핑한다.
2. 초평면과 각 class의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정 경계를 찾는다. 이 단계를 마진 최대화(margin maximization)이라고 한다. 

![[스크린샷 2025-02-06 오후 6.34.56.png]]


커널 기법
1. 새롭게 표현된 공간에서 좋은 결정 초평면을 찾기 위해 새로운 공간에 대응하는 데이터 포인트의 좌표를 실제로 구할 필요가 없다.
2. 새로운 공간에서의 두 데이터 포인트 사이의 거리를 게산 할 수만 있으면 된다.

SVM 장,단점
*장점*
1. 간단한 분류 문제에 대한 최고 수준 성능 달성
2. 수학적으로 깊게 분석하기 용이해서 이론을 이해하기 쉽다.

*단점*
1. SVM은 대용량 데이터셋에 확장되기 어렵다.
2. SVM은 얕은 학습 방법이기 때문에 지각에 관련된 문제에 SVM을 적용할려면 먼저 수동으로 유용한 표현을 추출해야 하는데 매우 어렵고 불안정하다.

#1.2.4 결정트리, 랜덤 포레스트, 그레이디언트 부스팅 머신
결정트리(decision tree)는 플로차트(flowchart) 같은 구조를 가지며 입력 데이터 포인트를 분류하거나 주어진 입력에 대해 출력 값을 에측한다.
![[스크린샷 2025-02-06 오후 7.18.50.png]]

랜덤 포레스트(Random Forest) 알고리즘은 결정 트리 학습에 기초한 것으로 안정적이고 실전에서 유용하다. -> 다양한 문제에 적용할 수 있다.

그레이디언트 부스팅 머신(gradient boosting machine)은 약한 예측 모델인 결정 트리를 앙상블 하는 것을 기반으로 하는 머신러닝 방법
-> 이전 모델에서 놓친 데이터 포인트를 보완하는 새로운 모델을 반복적으로 훈련함으로써 머신러닝 모델을 향상하는 방법인 gradient boosting방법을 사용한다.

#1.2.5 다시 신경망으로
제프리 힌튼(Geoffrey Hinton), 요수아 벤지오(Yoshua Bengio), 얀 르쿤, IDSIA
-> 신경망에 성과를 가져온 사람들

#1.2.6 딥러닝의 특징

머신러닝은 특성 공학(데이터의 좋은 표현을 수동으로 만드는 방법 == 이 방법을 특성공학이라고 함)을 통해 데이터를 처리한다.
but 딥러닝은 이 단계를 완전 자동화 한다.

딥러닝은 특성을 찾는 대신 한번에 모든 특성을 학습할 수 있다.

딥런닝의 주요 특징
1. 층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다.
2. 이런 점진적인 중간 표현이 공동으로 학습된다.

#1.2.7 머신러닝의 최근 동향

![[스크린샷 2025-02-10 오전 10.29.43.png]]![[스크린샷 2025-02-10 오전 10.31.16.png]]

머신러닝과 데이터 과학 업계에서 자주 사용된 2가지 방법
1. 딥러닝
2. 그레이디언트 부스티드

그레이디언트 부스티드 -> 구조적인 데이터를 가진 문제에서 사용
딥러닝 -> 이미지 분류 같은 지각 관련 문제

그레디언트 부스팅(Gradient Boosting)은 머신러닝에서 사용되는 앙상블 학습 기법 중 하나로, 예측 모델을 강력하게 만드는 데 효과적입니다. 이 방법은 여러 개의 약한 학습기(weak learner)를 순차적으로 결합하여 강력한 학습기를 만드는 방식으로 작동합니다.